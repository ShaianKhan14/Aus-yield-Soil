# -*- coding: utf-8 -*-
"""Copy of Aus_KNN(Fertilizer)_accuracy(Aus_Yield)_95%.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iaQQq026O2o05cE8Q-U121dhxqveoBIz
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import sklearn
from pandas import DataFrame
import plotly
from pandas_profiling import ProfileReport
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import MinMaxScaler
from numpy import math

from flask import Flask, request, jsonify, render_template
import pickle

df = pd.read_csv('Agricultural_Dataset.csv')
df.head()

df.isnull().sum()

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
df[['area']] = imputer.fit_transform(df[['area']])

df.head()

correlation_matrix = df.corr().round(2)
ax = plt.subplots(figsize=(25, 25))
# annot = True to print the values inside the square
sns.heatmap(data=correlation_matrix, annot=True)

data1 = df

data1 = data1.drop(['District'], axis=1)
data1 = data1.drop(['year'], axis=1)
data1 = data1.drop(['area'], axis=1)
data1 = data1.drop(['avg_rainfall'], axis=1)
data1 = data1.drop(['max_temperature'], axis=1)
data1 = data1.drop(['aman'], axis=1)
data1 = data1.drop(['boro'], axis=1)
data1 = data1.drop(['wheat'], axis=1)
data1 = data1.drop(['potato'], axis=1)
data1 = data1.drop(['jute'], axis=1)
data1 = data1.drop(['humidity'], axis=1)
data1 = data1.drop(['storm'], axis=1)
data1 = data1.drop(['inundationland_Highland'], axis=1)
data1 = data1.drop(['inundationland_mediumhighland'], axis=1)
data1 = data1.drop(['inundationland_lowland'], axis=1)
data1 = data1.drop(['inundationland_mediumlowland'], axis=1)
data1 = data1.drop(['inundationland_verylowland'], axis=1)
data1 = data1.drop(['Calcareous Alluvium'], axis=1)
data1 = data1.drop(['Noncalcareous Alluvium'], axis=1)
data1 = data1.drop(['Acid Basin Clay'], axis=1)
data1 = data1.drop(['Calcareous Brown Floodplain Soil'], axis=1)
data1 = data1.drop(['Calcareous Grey Floodplain Soil'], axis=1)
data1 = data1.drop(['Calcareous Dark Grey Floodplain Soil'], axis=1)
data1 = data1.drop(['Noncalcareous Grey Floodplain Soil'], axis=1)
data1 = data1.drop(['Peat'], axis=1)
data1 = data1.drop(['Made-Land'], axis=1)
data1 = data1.drop(['Shallow Red-Brown Terrace Soil'], axis=1)
data1 = data1.drop(['Deep Red-Brown Terrace Soil'], axis=1)
data1 = data1.drop(['Brown Mottled Terrace Soil'], axis=1)
data1 = data1.drop(['Shallow Grey Terrace Soil'], axis=1)
data1 = data1.drop(['Deep Grey Terrace Soil'], axis=1)
data1 = data1.drop(['Grey Valley Soil'], axis=1)

data1.head()

correlation_matrix = data1.corr().round(2)
ax = plt.subplots(figsize=(10, 10))
# annot = True to print the values inside the square
sns.heatmap(data=correlation_matrix, annot=True, )

dependent = 'aus'

independent = data1.columns.tolist()
 independent.remove(dependent)
 independent

X = df[independent]
Y = df[dependent]

from sklearn.model_selection import train_test_split,cross_validate

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)

print(X_train.shape)
print(X_test.shape)

"""K-Nearest Neighbor Algorithm"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn import metrics
#Setup arrays to store training and test accuracies
neighbors = np.arange(1,9)
train_accuracy =np.empty(len(neighbors))
test_accuracy = np.empty(len(neighbors))

classifier = KNeighborsRegressor(n_neighbors=5)
classifier.fit(X_train, Y_train)

"""Accuracy"""

accuracy = classifier.score(X_test, Y_test)*100
accuracy

accuracy = classifier.score(X_train, Y_train)*100
accuracy

#Compute accuracy on the training set
train_accuracy[1] = classifier.score(X_train, Y_train)
train_accuracy

#Compute accuracy on the test set
test_accuracy[1] = classifier.score(X_test, Y_test) 
test_accuracy

plotdata = pd.DataFrame({
    "Accuracy For KNN":[92.3635308536599, 95.48367990862825],
    
    }, 
    index=["Train_Accuracy", "Test_Accuracy", ]
)
plotdata.plot(kind="bar")
plt.title("Comparison between accuracy",fontsize = 14)
fontsize = 14
fontweight = 'bold'
fontproperties = {'family':'sans-serif','sans-serif':['Helvetica'],'weight' : fontweight, 'size' : fontsize}
plt.xlabel("Accuracy",fontsize = 14)
plt.ylabel("Accuracy_Values",fontsize = 14)

Y_pred = classifier.predict(X_test)

plt.scatter(Y_test, Y_pred)
fontsize = 14
fontweight = 'bold'
fontproperties = {'family':'sans-serif','sans-serif':['Helvetica'],'weight' : fontweight, 'size' : fontsize}
plt.xlabel("Actual value(LR)",fontsize = 14)
plt.ylabel("Predicted value",fontsize = 14)
plt.title("Actual value vs Predicted value",fontsize = 14)
plt.show

"""Root_Mean_Squre_Error"""

rmse = np.sqrt(np.mean((Y_test - Y_pred)**2))

print("RMSE : {:.2f}".format(rmse))

"""Mean_absolute_error"""

# Use the forest's predict method on the test data
Y_pred = classifier.predict(X_test)
# Calculate the absolute errors
errors = abs(Y_pred - Y_test)
# Print out the mean absolute error (mae)
print('Mean Absolute Error:', round(np.mean(errors), 2))

from sklearn.metrics import mean_absolute_error, mean_squared_error

mse = mean_squared_error(Y_test, Y_pred)
mae = mean_absolute_error(Y_test, Y_pred) 

print('Mean_Absolute_Error: %2f'% mae)
print('Mean_Squared_Error: %2f'% mse)

# Mean Absolute Percentage Error (MAPE)
MAPE = np.mean((np.abs(np.subtract(Y_test, Y_pred)/ Y_test))) * 100
print('Mean Absolute Percentage Error (MAPE): ' + str(np.round(MAPE, 2)) + ' %')

plotdata = pd.DataFrame({
    "Model Error For KNN":[6470.12, 35.33 ,  3503.01 ],
    
    }, 
    index=["RMSE", "MAPE", "MAE"]
)
plotdata.plot(kind="bar")
plt.title("Comparison between error",fontsize = 14)
fontsize = 14
fontweight = 'bold'
fontproperties = {'family':'sans-serif','sans-serif':['Helvetica'],'weight' : fontweight, 'size' : fontsize}
plt.xlabel("Error Metrics",fontsize = 14)
plt.ylabel("Error Value",fontsize = 14)

# Saving model to disk
pickle.dump(KNeighborsRegressor, open('model.pkl','wb'))

# Loading model to compare the results
model = pickle.load(open('model.pkl','rb'))



